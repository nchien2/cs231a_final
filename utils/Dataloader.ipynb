{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25899,"status":"ok","timestamp":1679404158696,"user":{"displayName":"Clara Lynn Everett","userId":"17000400504877492044"},"user_tz":0},"id":"ztAlFI36Wdzy","outputId":"22438b8f-a8a4-4214-852b-34000a13337a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8U4wewAdXDm"},"outputs":[],"source":["# AUGMENTATIONS\n","import random\n","import math\n","import torch\n","\n","# Any argument named keypoints should be a 1D tensor of keypoint values, eg keypoint_json['pose_keypoints_2d']\n","\n","def keypoint_to_coord(keypoints):\n","  keypoints = keypoints.reshape(-1, 3)\n","  uncertainty = keypoints[:,2]\n","  x = keypoints[:,0]\n","  y = keypoints[:,1]\n","  return x, y, uncertainty\n","\n","def coord_to_keypoint(x, y, uncertainty):\n","  coord_tensor = torch.stack((x, y, uncertainty), dim=1)\n","  keypoints = coord_tensor.reshape(-1)\n","  return keypoints\n","\n","# Rotate around center by a random angle\n","def rotation(keypoints):\n","  angle = random.uniform(-0.15, 0.15)\n","  x, y, uncertainties = keypoint_to_coord(keypoints)\n","  \n","  x_rot = (x - 0.5) * math.cos(angle) - (y - 0.5) * math.sin(angle) + 0.5\n","  y_rot = (y - 0.5) * math.cos(angle) - (x - 0.5) * math.sin(angle) + 0.5\n","  keypoints = coord_to_keypoint(x_rot, y_rot, uncertainties)\n","  return keypoints\n","\n","# \n","def squeeze(keypoints):\n","  x, y, uncertainties = keypoint_to_coord(keypoints)\n","\n","  width = torch.max(x) - torch.min(x)\n","  left_squeeze = random.uniform(0, 0.15) * width\n","  right_squeeze = random.uniform(0, 0.15) * width\n","  x = (x - left_squeeze) / (width - left_squeeze - right_squeeze)\n","  keypoints = coord_to_keypoint(x, y, uncertainties)\n","  return keypoints\n","\n","def projection(keypoints):\n","  x, y, uncertainties = keypoint_to_coord(keypoints)\n","  rot_angle = np.random.uniform(-0.15, 0.15)\n","  Tx = np.random.uniform(-0.15, 0.15)\n","  Ty = np.random.uniform(-0.15, 0.15)\n","  R = torch.tensor([\n","        [math.cos(rot_angle), -math.sin(rot_angle), Tx],\n","        [math.sin(rot_angle), math.cos(rot_angle),Ty],\n","        [0, 0, 1]\n","  ])\n","\n","  Sx = np.random.uniform(-0.1, 0.1)\n","  Sy = np.random.uniform(-0.1, 0.1)\n","  A = torch.tensor([\n","    [1, Sy, 0],\n","    [Sx, 1, 0],\n","    [0,0,1]\n","  ])\n","\n","  p1 = np.random.uniform(-0.0001, 0.0001)\n","  p2 = np.random.uniform(-0.0001, 0.0001)\n","  P = torch.tensor([\n","      [1, 0, 0],\n","      [0, 1, 0],\n","      [p1,p2,1]\n","  ])\n","  H = R @ A @ P \n","  coords = torch.stack((x, y, torch.ones(x.shape[0])))\n","  new_coords = H @ coords\n","  new_coords = new_coords[:2, :] / new_coords[2,:]\n","  x = new_coords[0, :]\n","  y = new_coords[1, :]\n","  keypoints_new = coord_to_keypoint(x, y, uncertainties)\n","  return keypoints_new\n","\n","\n","def augment(keypoints):\n","  p = np.random.randint(0, 3)\n","  if p == 0:\n","    keypoints = rotation(keypoints)\n","  elif p == 1:\n","    keypoints = squeeze(keypoints)\n","  elif p == 2:\n","    keypoints = projection(keypoints)\n","  return keypoints"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Lk45lXngCZ9"},"outputs":[],"source":["\n","import json\n","import pandas as pd\n","\n","\n","# NORMALIZATION CODE =============================================\n","def normalize_helper(keypoints):\n","  x, y, uncertainties = keypoint_to_coord(keypoints)\n","  x = x.float()\n","  y = y.float()\n","\n","  x = (x - torch.mean(x)) / torch.std(x)\n","  y = (y - torch.mean(y)) / torch.std(y)\n","\n","  keypoints = coord_to_keypoint(x, y, uncertainties)\n","  return keypoints\n","\n","\n","def normalize_keypoints(kp_json):\n","  left_hand = torch.tensor(kp_json['hand_left_keypoints_2d'])\n","  right_hand = torch.tensor(kp_json['hand_right_keypoints_2d'])\n","  face = torch.tensor(kp_json['face_keypoints_2d'])\n","  body = torch.tensor(kp_json['pose_keypoints_2d'])\n","\n","  normalized = {}\n","  normalized['hand_left_keypoints_2d'] = normalize_helper(left_hand).tolist()\n","  normalized['hand_right_keypoints_2d'] = normalize_helper(right_hand).tolist()\n","  normalized['face_keypoints_2d'] = normalize_helper(face).tolist()\n","  normalized['pose_keypoints_2d'] = normalize_helper(body).tolist()\n","\n","  return normalized\n","\n","\n","# LOOP THROUGH ALL FILES AND NORMALIZE FOR PRE-PROCESSING\n","\n","def normalize_all():\n","  annotations_file = 'drive/MyDrive/CS231A/asllvd_signs_2023_02_16.csv'\n","  pose_dir = 'drive/MyDrive/CS231A/ASLLVD/output_json/'\n","  output_dir = 'drive/MyDrive/CS231A/ASLLVD/normalized_json'\n","\n","  df = pd.read_csv(annotations_file)\n","  df = df.iloc[9500:]\n","\n","\n","  for idx, row in df.iterrows():\n","    pose_path = os.path.join(pose_dir, f'{idx}')\n","    output_path = os.path.join(output_dir, f'{idx}')\n","\n","    if not os.path.isdir(pose_path):\n","      continue\n","    elif not os.path.isdir(output_path):\n","      os.makedirs(output_path)\n","\n","    for filename in os.listdir(pose_path):\n","      json_path = os.path.join(pose_path, filename)\n","      f = open(json_path)\n","      kp_json = json.load(f)['people'][0]\n","      kp_json = normalize_keypoints(kp_json)\n","\n","      with open(os.path.join(output_path, filename),'w') as output_f:\n","        json.dump(kp_json, output_f, ensure_ascii=False, indent=4)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ekV4efdUmTa1"},"outputs":[],"source":["# normalize_all()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FGlR_cNAQwHH"},"outputs":[],"source":["# DATALOADER \n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import json\n","import os\n","import pandas as pd\n","\n","\n","# TODO: Add keep_uncertainty flag\n","# TODO: Add explicit stratification\n","class ASLLVDataset(Dataset):\n","  def __init__(self, df, pose_dir, \n","               transform=None, \n","               keep_uncertainty=True,\n","               normalization=True, \n","               train=True):\n","      self.df = df\n","      self.pose_dir = pose_dir\n","      self.transform = transform\n","      self.keep_uncertainty = keep_uncertainty\n","\n","  def __len__(self):\n","      return len(self.df)\n","\n","  def __getitem__(self, idx):     \n","      pose_path = os.path.join(self.pose_dir, f'{idx}')\n","\n","      frame_keypoints = []\n","      for filename in os.listdir(pose_path):\n","        # print(f'filename: {filename}')\n","        json_path = os.path.join(pose_path, filename)\n","        f = open(json_path)\n","        kp_json = json.load(f)['people'][0]\n","        kp_list = [\n","            kp_json['pose_keypoints_2d'],\n","            kp_json['face_keypoints_2d'],\n","            kp_json['hand_left_keypoints_2d'],\n","            kp_json['hand_right_keypoints_2d']\n","        ]\n","        kp_list = [torch.tensor(x) for x in kp_list]\n","        keypoint_tensor = torch.cat(kp_list)\n","\n","        if self.train and self.transform:\n","          keypoint_tensor = augment(keypoints)\n","\n","\n","        # print('KEYPOINTS==========')\n","        # print(keypoint_tensor)\n","        # print('Xs===============')\n","        # x, y, un = keypoint_to_coord(keypoint_tensor)\n","        # print(x)\n","        # print('KEYPOINTS===============')\n","        # print(coord_to_keypoint(x, y, un))\n","\n","        if not self.keep_uncertainty:\n","          keypoint_tensor = keypoint_tensor.reshape(-1, 3)[:,:2].reshape(-1)\n","\n","        # print(f'keypoint_tensor: {keypoint_tensor.shape}')\n","        frame_keypoints.append(keypoint_tensor)\n","      full_pose = torch.stack(frame_keypoints)\n","      # print(f'full pose: {full_pose.shape}')\n","      \n","      label = self.df['main entry gloss label'].iloc[idx]\n","\n","      if self.transform:\n","          full_pose = self.transform(full_pose)\n","\n","      return full_pose, label\n","\n","def collate_fn(data):\n","  poses, labels = zip(*data)\n","  return poses, labels\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXcjndpbvbXB"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QzD1LMBWWB_j","colab":{"base_uri":"https://localhost:8080/","height":393},"executionInfo":{"status":"error","timestamp":1679404161975,"user_tz":0,"elapsed":619,"user":{"displayName":"Clara Lynn Everett","userId":"17000400504877492044"}},"outputId":"b8499e91-8f22-4768-a580-66f1e82675ff"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-6d9fa26e195b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'main entry gloss label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'categorical_label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'main entry gloss label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'categorical_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'categorical_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2581\u001b[0m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCVClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2583\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2585\u001b[0m     return list(\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \"\"\"\n\u001b[1;32m   1688\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1689\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1690\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2076\u001b[0m         \u001b[0mclass_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_counts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2078\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2079\u001b[0m                 \u001b[0;34m\"The least populated class in y has only 1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m                 \u001b[0;34m\" member, which is too few. The minimum\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."]}],"source":["from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","\n","\n","batch_size = 1\n","shuffle = True\n","annotations_file = 'drive/MyDrive/CS231A/asllvd_signs_2023_02_16.csv'\n","pose_dir = 'drive/MyDrive/CS231A/ASLLVD/output_json/'\n","normal_pose_dir = 'drive/MyDrive/CS231A/ASLLVD/normalized_json/'\n","\n","#TODO: TEST THAT SPLIT WORKS \n","# sTRATIFY PROBABLY BREAKS, REMOVE GLOSSES WITH < 3 EXAMPLES\n","df = pd.read_csv(annotations_file)\n","le = preprocessing.LabelEncoder()\n","le.fit(df['main entry gloss label'])\n","df['categorical_label'] = le.transform(df['main entry gloss label'])\n","train, val_test = train_test_split(df, test_size=0.3, stratify=df['categorical_label'])\n","val, test = train_test_split(val_test, test_size=0.5, stratify=val_test['categorical_label'])\n","\n","train_ds = ASLLVDataset(train, pose_dir, keep_uncertainty=True)\n","val_ds = ASLLVDataset(val, pose_dir, keep_uncertainty=True)\n","test_ds = ASLLVDataset(test, pose_dir, keep_uncertainty=True)\n","\n","train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n","val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n","test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n"]},{"cell_type":"code","source":["df = pd.read_csv(annotations_file)\n","df = df.loc[:5]\n","\n","ds = ASLLVDataset(df, pose_dir, keep_uncertainty=True)\n","dl = DataLoader(ds, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)"],"metadata":{"id":"AJ_W8UX27cd4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F3E-BeFFuOwb"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"txuuj8GAdQGB"},"outputs":[],"source":["for idx, batch in enumerate(train_dl):\n","  # Each output has shape (batch_size, #frames, keypoint_dim)\n","  print(batch)\n","  print(batch[0].shape)\n","  print(batch[1])\n","  break\n","  # print(f'{idx}: }')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5dwt4J_HowE3"},"outputs":[],"source":["%ls drive/MyDrive/CS231A/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ioj6-O15pCtQ"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}